# -*- coding: utf-8 -*-
"""ml_assignment_regression_ghaida104

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W9xpeFYPg3pUPxw2jWnR342fsjiZJ-1Z
"""

import pandas as pd

# قراءة البيانات
df = pd.read_csv("StudentsPerformance.csv")

# عرض أول 5 صفوف
df.head()

df.shape

# أنواع الأعمدة وهل فيها بيانات ناقصة
df.info()

# إحصائيات عن الأعمدة الرقمية
df.describe()

#هل في بيانات ناقصة؟
df.isnull().sum()

#نرسم رسم بياني يبين توزيع درجات الطلاب بالرياضيات
import matplotlib.pyplot as plt

plt.hist(df['math score'], bins=20, edgecolor='black')
plt.title('Math Score Distribution')
plt.xlabel('Score')
plt.ylabel('Number of Students')
plt.show()

# نرسم العلاقة بين درجات الرياضيات والقراءة
plt.scatter(df['math score'], df['reading score'])
plt.title('Math vs Reading Score')
plt.xlabel('Math Score')
plt.ylabel('Reading Score')
plt.show()

# نختار عمود الهدف
target = 'math score'

# نحط المتغيرات اللي بنتنبأ منها في X، ونشيل عمود الهدف
X = df.drop(columns=[target])
y = df[target]

# تحويل الأعمدة النصية إلى أرقام باستخدام one-hot encoding
X_encoded = pd.get_dummies(X, drop_first=True)

# التأكد من الشكل النهائي للبيانات
X_encoded.head()

from sklearn.model_selection import train_test_split

# نقسم البيانات: 80% تدريب و 20% اختبار
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# نطبع شكل المجموعات للتأكد
X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# إنشاء النموذج
model = LinearRegression()

# تدريب النموذج
model.fit(X_train, y_train)

# التنبؤ على بيانات الاختبار
y_pred = model.predict(X_test)


# تقييم الأداء
mse = mean_squared_error(y_test, y_pred) # ( لحقيقي - التوق)^2\ =    كل ما كان اقل كل ما كان توقعه 100%
r2 = r2_score(y_test, y_pred)  #coefficient determination

print("Mean Squared Error:", mse)
print("R² Score:", r2)

import numpy as np
import matplotlib.pyplot as plt

# تحويل البيانات إلى numpy arrays عشان نشتغل يدوي بدون مكتبه
X_np = X_train.values
y_np = y_train.values.reshape(-1, 1)

# نضيف عمود 1 للبداية (للـ bias)
X_np = np.hstack([np.ones((X_np.shape[0], 1)), X_np])

# تأكدي إن البيانات كلها أرقام عشرية (float)
X_np = X_np.astype(float)
y_np = y_np.astype(float)

# دالة لحساب J (cost)
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2) # فيه قسمه على اثنين
    return cost

# دالة تدريب النموذج باستخدام gradient descent
def gradient_descent(X, y, alpha, iterations):
    m = len(y)
    theta = np.zeros((X.shape[1], 1)) # كل الاوزان صفر
    cost_history = []

    for _ in range(iterations): #1000
        predictions = X.dot(theta)
        error = predictions - y
        gradient = (1 / m) * X.T.dot(error) # حساب القريدينت \\زي المسافه بين التوقع و الجواب الصحيح لو كان صفر يعني لمسنا الحل الصح
        theta -= alpha * gradient
        cost_history.append(compute_cost(X, y, theta))

    return theta, cost_history

# نستخدم alpha = 0.01 ونجرب التدريب لمدة 1000 تكرار
alpha = 0.01
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

# نرسم تغير الكوست مع التكرارات
plt.plot(range(iterations), cost_history)
plt.title(f"Learning Rate = {alpha}")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

alpha = 0.001
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

plt.plot(range(iterations), cost_history)
plt.title(f"Learning Rate = {alpha}")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

alpha = 0.1
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

plt.plot(range(iterations), cost_history)
plt.title("Learning Rate = 0.1")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

alpha = 0.0001
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

plt.plot(range(iterations), cost_history)
plt.title("Learning Rate = 0.0001")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

alpha = 1
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

plt.plot(range(iterations), cost_history)
plt.title("Learning Rate = 1")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

alpha = 10
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

plt.plot(range(iterations), cost_history)
plt.title("Learning Rate = 10")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

alpha = 100
iterations = 1000

theta, cost_history = gradient_descent(X_np, y_np, alpha, iterations)

plt.plot(range(iterations), cost_history)
plt.title("Learning Rate = 100")
plt.xlabel("Iterations")
plt.ylabel("Cost (J)")
plt.grid(True)
plt.show()

from sklearn.linear_model import Ridge
# L2 regularization
# نجرّب Regularization باستخدام Ridge
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)

# التنبؤ
y_pred_ridge = ridge_model.predict(X_test)

# التقييم
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

print("Ridge Regression:")
print("Mean Squared Error:", mse_ridge)
print("R² Score:", r2_ridge)

# عرض أوزان الموديل لكل ميزة
for feature, weight in zip(X_train.columns, ridge_model.coef_):
    print(f"{feature}: {weight}")

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# إنشاء ميزات جديدة من الدرجة الثانية
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train)

# تدريب النموذج على البيانات المتعددة الحدود
poly_model = LinearRegression()
poly_model.fit(X_poly, y_train)

# تحويل بيانات الاختبار وتوقّع النتائج
X_test_poly = poly.transform(X_test)
y_pred_poly = poly_model.predict(X_test_poly)

# التقييم
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

print("Polynomial Regression (degree=2):")
print("Mean Squared Error:", mse_poly)
print("R² Score:", r2_poly)





