# -*- coding: utf-8 -*-
"""Classification_Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tzKRThoWQybsb-Q5Job6sai-I44tNyoX
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('StudentsPerformance.csv')

# Create the binary target column: 1 = Pass, 0 = Fail
df['target'] = df.apply(
    lambda row: 1 if row['math score'] >= 50 and row['reading score'] >= 50 and row['writing score'] >= 50 else 0,
    axis=1
)

# Show the value counts of the target
print("Target class distribution (0 = Fail, 1 = Pass):")
print(df['target'].value_counts())
print("\nPercentages:")
print(df['target'].value_counts(normalize=True) * 100)

import matplotlib.pyplot as plt

# Count values
counts = df['target'].value_counts()
percentages = df['target'].value_counts(normalize=True) * 100

# Plot
plt.figure(figsize=(6, 4))
ax = counts.plot(kind='bar', color=['red', 'green'])

# Title and labels
plt.title('Pass vs Fail Distribution')
plt.xticks([0, 1], ['Pass (1)', 'Fail (0)'], rotation=0)
plt.ylabel('Number of Students')
plt.xlabel('Class')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add value + percentage on top of bars
for i, (count, percent) in enumerate(zip(counts, percentages)):
    label = f'{count} â€” {percent:.1f}%'
    plt.text(i, count + 10, label, ha='center', fontsize=10, color='black')

# Save the figure (optional)
plt.savefig('class_distribution_labeled.png', bbox_inches='tight')

# Show plot
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Plot histogram for math score
plt.figure(figsize=(7, 4))
sns.histplot(data=df, x='math score', bins=20, kde=True, color='skyblue')

plt.title('Distribution of Math Scores')
plt.xlabel('Math Score')
plt.ylabel('Number of Students')
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.savefig('hist_math_score.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(7, 4))
sns.histplot(data=df, x='reading score', bins=20, kde=True, color='lightgreen')

plt.title('Distribution of Reading Scores')
plt.xlabel('Reading Score')
plt.ylabel('Number of Students')
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.savefig('hist_reading_score.png', bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create a histogram for writing score
plt.figure(figsize=(7, 4))

# Plot histogram with KDE (Kernel Density Estimate) curve
sns.histplot(data=df, x='writing score', bins=20, kde=True, color='plum')

# Set plot title and axis labels
plt.title('Distribution of Writing Scores')
plt.xlabel('Writing Score')
plt.ylabel('Number of Students')

# Add gridlines for better readability
plt.grid(axis='y', linestyle='--', alpha=0.5)

# Save the figure as an image file
plt.savefig('hist_writing_score.png', bbox_inches='tight')

# Display the plot
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a boxplot for math score by target class (Pass/Fail)
plt.figure(figsize=(6, 4))
sns.boxplot(data=df, x='target', y='math score', palette='Set2')

# Set plot title and labels
plt.title('Math Score by Target Class')
plt.xlabel('Target (0 = Fail, 1 = Pass)')
plt.ylabel('Math Score')

# Save the figure
plt.savefig('boxplot_math_score.png', bbox_inches='tight')

# Show the plot
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a boxplot for reading score by target class (Pass/Fail)
plt.figure(figsize=(6, 4))
sns.boxplot(data=df, x='target', y='reading score', palette='Set2')

# Set plot title and axis labels
plt.title('Reading Score by Target Class')
plt.xlabel('Target (0 = Fail, 1 = Pass)')
plt.ylabel('Reading Score')

# Save the figure as an image file
plt.savefig('boxplot_reading_score.png', bbox_inches='tight')

# Show the plot
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a boxplot for writing score by target class (Pass/Fail)
plt.figure(figsize=(6, 4))
sns.boxplot(data=df, x='target', y='writing score', palette='Set2')

# Set plot title and axis labels
plt.title('Writing Score by Target Class')
plt.xlabel('Target (0 = Fail, 1 = Pass)')
plt.ylabel('Writing Score')

# Save the figure as an image file
plt.savefig('boxplot_writing_score.png', bbox_inches='tight')

# Show the plot
plt.show()

from sklearn.model_selection import train_test_split

# Step 1: Select the numeric features only
features = ['math score', 'reading score', 'writing score']
X = df[features]
y = df['target']

# Step 2: Split the data into train and test sets (80% train, 20% test)
# We use stratify=y to keep the class distribution balanced
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 3: Show the shape of each set
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from sklearn.preprocessing import StandardScaler

# Step 1: Create the scaler
scaler = StandardScaler()

# Step 2: Fit the scaler on training data, then transform both train and test
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Show the shape to confirm it's still correct
print("Scaled X_train shape:", X_train_scaled.shape)
print("Scaled X_test shape:", X_test_scaled.shape)

import numpy as np

# Step 1: Define the sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Step 2: Define the logistic loss function (binary cross-entropy)
def compute_loss(y_true, y_pred):
    # Add a small value to avoid log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Step 3: Train logistic regression using gradient descent
def train_logistic_regression(X, y, lr=0.1, epochs=1000):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    losses = []

    for epoch in range(epochs):
        # Linear combination
        z = np.dot(X, weights) + bias
        # Apply sigmoid to get prediction
        y_pred = sigmoid(z)

        # Compute gradients
        dw = np.dot(X.T, (y_pred - y)) / n_samples
        db = np.mean(y_pred - y)

        # Update weights
        weights -= lr * dw
        bias -= lr * db

        # Save loss
        if epoch % 100 == 0:
            loss = compute_loss(y, y_pred)
            losses.append(loss)

    return weights, bias, losses

# Step 4: Predict function
def predict(X, weights, bias, threshold=0.5):
    probs = sigmoid(np.dot(X, weights) + bias)
    return (probs >= threshold).astype(int)

from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Train the model using our custom function (no regularization)
weights, bias, losses = train_logistic_regression(X_train_scaled, y_train.values, lr=0.1, epochs=1000)

# Step 2: Make predictions on the test set
y_pred_test = predict(X_test_scaled, weights, bias)

# Step 3: Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_test)
print("Test Accuracy (no regularization):", round(accuracy * 100, 2), "%")

# Step 4: Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:\n", conf_matrix)

# Train logistic regression with L2 regularization
def train_logistic_regression_L2(X, y, lr=0.1, epochs=1000, lambda_=0.1):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for epoch in range(epochs):
        z = np.dot(X, weights) + bias
        y_pred = sigmoid(z)

        # Compute gradients with L2 regularization
        dw = (np.dot(X.T, (y_pred - y)) / n_samples) + (lambda_ * weights)
        db = np.mean(y_pred - y)

        # Update weights and bias
        weights -= lr * dw
        bias -= lr * db

    return weights, bias

# Train with L2 regularization
weights_l2, bias_l2 = train_logistic_regression_L2(X_train_scaled, y_train.values, lr=0.1, epochs=1000, lambda_=0.1)

# Predict on test set
y_pred_l2 = predict(X_test_scaled, weights_l2, bias_l2)

# Evaluate
accuracy_l2 = accuracy_score(y_test, y_pred_l2)
conf_matrix_l2 = confusion_matrix(y_test, y_pred_l2)

print("Test Accuracy (with L2 regularization):", round(accuracy_l2 * 100, 2), "%")
print("Confusion Matrix:\n", conf_matrix_l2)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Create and train the model
# We add class_weight='balanced' in case of imbalance, but you can remove it if you want
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_train_scaled, y_train)

# Step 2: Predict on test set
y_pred_sklearn = model.predict(X_test_scaled)

# Step 3: Evaluate
accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)
conf_matrix_sklearn = confusion_matrix(y_test, y_pred_sklearn)

print("Test Accuracy (sklearn logistic regression):", round(accuracy_sklearn * 100, 2), "%")
print("Confusion Matrix:\n", conf_matrix_sklearn)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Create and train the model
# We add class_weight='balanced' in case of imbalance, but you can remove it if you want
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_train_scaled, y_train)

# Step 2: Predict on test set
y_pred_sklearn = model.predict(X_test_scaled)

# Step 3: Evaluate
accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)
conf_matrix_sklearn = confusion_matrix(y_test, y_pred_sklearn)

print("Test Accuracy (sklearn logistic regression):", round(accuracy_sklearn * 100, 2), "%")
print("Confusion Matrix:\n", conf_matrix_sklearn)

import numpy as np

# Step 1: Define the perceptron training function
def train_perceptron(X, y, lr=0.01, epochs=1000):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for _ in range(epochs):
        for i in range(n_samples):
            linear_output = np.dot(X[i], weights) + bias
            y_pred = 1 if linear_output > 0 else 0

            # Update rule
            error = y[i] - y_pred
            weights += lr * error * X[i]
            bias += lr * error

    return weights, bias

# Step 2: Prediction function
def predict_perceptron(X, weights, bias):
    linear_output = np.dot(X, weights) + bias
    return np.where(linear_output > 0, 1, 0)

from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Train the perceptron
w_p, b_p = train_perceptron(X_train_scaled, y_train.values, lr=0.01, epochs=1000)

# Step 2: Predict on test set
y_pred_p = predict_perceptron(X_test_scaled, w_p, b_p)

# Step 3: Evaluate
accuracy_p = accuracy_score(y_test, y_pred_p)
conf_matrix_p = confusion_matrix(y_test, y_pred_p)

print("Test Accuracy (Perceptron):", round(accuracy_p * 100, 2), "%")
print("Confusion Matrix:\n", conf_matrix_p)

import numpy as np

# Step 1: Fit the Naive Bayes model (calculate mean, var for each class)
def fit_naive_bayes(X, y):
    classes = np.unique(y)
    summaries = {}

    for c in classes:
        X_c = X[y == c]
        summaries[c] = {
            "mean": X_c.mean(axis=0),
            "var": X_c.var(axis=0),
            "prior": X_c.shape[0] / X.shape[0]
        }

    return summaries

# Step 2: Calculate Gaussian probability density
def gaussian_pdf(x, mean, var):
    eps = 1e-6  # to avoid division by zero
    coef = 1.0 / np.sqrt(2.0 * np.pi * var + eps)
    exponent = np.exp(- ((x - mean) ** 2) / (2 * var + eps))
    return coef * exponent

# Step 3: Predict class for each sample
def predict_naive_bayes(X, summaries):
    y_pred = []
    for x in X:
        posteriors = {}
        for c, params in summaries.items():
            prior = np.log(params["prior"])
            class_likelihood = np.sum(np.log(gaussian_pdf(x, params["mean"], params["var"])))
            posteriors[c] = prior + class_likelihood
        y_pred.append(max(posteriors, key=posteriors.get))
    return np.array(y_pred)

from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Fit the model
nb_model = fit_naive_bayes(X_train_scaled, y_train.values)

# Step 2: Predict on the test set
y_pred_nb = predict_naive_bayes(X_test_scaled, nb_model)

# Step 3: Evaluate the model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)

print("Test Accuracy (Naive Bayes):", round(accuracy_nb * 100, 2), "%")
print("Confusion Matrix:\n", conf_matrix_nb)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

# Step 1: Create and train the SVM model
svm_model = SVC(kernel='linear', class_weight='balanced')
svm_model.fit(X_train_scaled, y_train)

# Step 2: Predict on the test set
y_pred_svm = svm_model.predict(X_test_scaled)

# Step 3: Evaluate the model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)

print("Test Accuracy (SVM):", round(accuracy_svm * 100, 2), "%")
print("Confusion Matrix:\n", conf_matrix_svm)

